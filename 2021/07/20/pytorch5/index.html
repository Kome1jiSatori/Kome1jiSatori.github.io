<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kome1jisatori.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="前言循环神经网络原理学习应用">
<meta property="og:type" content="article">
<meta property="og:title" content="循环神经网络（RNN）原理和简单应用">
<meta property="og:url" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/index.html">
<meta property="og:site_name" content="KomeijiSatoriのblog">
<meta property="og:description" content="前言循环神经网络原理学习应用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/RNN%E5%9B%BE.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/RNN%E5%B1%95%E5%BC%80.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/%E5%9F%BA%E7%A1%80%E7%9A%84RNN%E5%B1%95%E5%BC%80%E5%9B%BE.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/RNN%E5%8A%9F%E8%83%BD.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/LSTM1.jpg">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/LSTM2.jpg">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/LSTM3.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/LSTM4.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/%E6%98%93%E7%8E%8B%E9%97%A8.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/%E8%BE%93%E5%85%A5%E9%97%A8.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/LSTM-update.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/%E8%BE%93%E5%87%BA%E9%97%A8.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/GRU.png">
<meta property="og:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/bidir_lstm.png">
<meta property="article:published_time" content="2021-07-20T17:16:50.000Z">
<meta property="article:modified_time" content="2021-07-21T09:41:45.955Z">
<meta property="article:author" content="Komeiji Satori">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kome1jisatori.github.io/2021/07/20/pytorch5/RNN%E5%9B%BE.png">

<link rel="canonical" href="https://kome1jisatori.github.io/2021/07/20/pytorch5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>循环神经网络（RNN）原理和简单应用 | KomeijiSatoriのblog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">KomeijiSatoriのblog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">地灵殿从零开始的学习生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">4</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">26</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://kome1jisatori.github.io/2021/07/20/pytorch5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Komeiji Satori">
      <meta itemprop="description" content="记录些有的没的的学习心得">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KomeijiSatoriのblog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          循环神经网络（RNN）原理和简单应用
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-07-21 01:16:50 / 修改时间：17:41:45" itemprop="dateCreated datePublished" datetime="2021-07-21T01:16:50+08:00">2021-07-21</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a>
                </span>
            </span>

          
            <span id="/2021/07/20/pytorch5/" class="post-meta-item leancloud_visitors" data-flag-title="循环神经网络（RNN）原理和简单应用" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/07/20/pytorch5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/07/20/pytorch5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
     
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>循环神经网络原理学习应用</p>
<span id="more"></span> 

<h1 id="循环神经网络原理"><a href="#循环神经网络原理" class="headerlink" title="循环神经网络原理"></a>循环神经网络原理</h1><h2 id="1-循环神经网络的介绍"><a href="#1-循环神经网络的介绍" class="headerlink" title="1. 循环神经网络的介绍"></a>1. 循环神经网络的介绍</h2><p>在普通的神经网络中，信息的传递是单向的，这种限制虽然使得网络变得更容易学习，但在一定程度上也减弱了神经网络模型的能力。特别是在很多现实任务中，网络的输出不仅和当前时刻的输入相关，也和其过去一段时间的输出相关。此外，普通网络难以处理时序数据，比如视频、语音、文本等，时序数据的长度一般是不固定的，而前馈神经网络要求输入和输出的维数都是固定的，不能任意改变。因此，当处理这一类和时序相关的问题时，就需要一种能力更强的模型。</p>
<p>循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络。在循环神经网络中，神经元不但可以接受其它神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。换句话说：神经元的输出可以在下一个时间步直接作用到自身</p>
<p><img src="/2021/07/20/pytorch5/RNN%E5%9B%BE.png"></p>
<p>通过简化图，我们看到RNN比传统的神经网络多了一个循环圈，这个循环表示的就是在下一个时间步（<strong>Time Step</strong>）上会返回作为输入的一部分，我们把RNN在时间点上展开，得到的图形如下：</p>
<p><img src="/2021/07/20/pytorch5/RNN%E5%B1%95%E5%BC%80.png"></p>
<p>或者是：</p>
<p><img src="/2021/07/20/pytorch5/%E5%9F%BA%E7%A1%80%E7%9A%84RNN%E5%B1%95%E5%BC%80%E5%9B%BE.png"></p>
<p>在不同的时间步，RNN的输入都将与之前的时间状态有关，$t_n$时刻网络的输出结果是该时刻的输入和所有历史共同作用的结果，这就达到了对时间序列建模的目的。</p>
<p>RNN的不同表示和功能可以通过下图看出：</p>
<p><img src="/2021/07/20/pytorch5/RNN%E5%8A%9F%E8%83%BD.png"></p>
<ul>
<li>图1：固定长度的输入和输出 (e.g. 图像分类)</li>
<li>图2：序列输出 (e.g.图像转文字)</li>
<li>图3：数列输入 (e.g. 文本分类)</li>
<li>图4：异步的序列输入和输出(e.g.文本翻译).</li>
<li>图5：同步的序列输入和输出 (e.g. 根据视频的每一帧来对视频进行分类)</li>
</ul>
<h2 id="2-LSTM和GRU"><a href="#2-LSTM和GRU" class="headerlink" title="2. LSTM和GRU"></a>2. LSTM和GRU</h2><h3 id="2-1-LSTM的基础介绍"><a href="#2-1-LSTM的基础介绍" class="headerlink" title="2.1 LSTM的基础介绍"></a>2.1 LSTM的基础介绍</h3><p>假如现在有这样一个需求，根据现有文本预测下一个词语，比如<code>天上的云朵漂浮在__</code>，通过间隔不远的位置就可以预测出来词语是<code>天上</code>，但是对于其他一些句子，可能需要被预测的词语在前100个词语之前，那么此时由于间隔非常大，随着间隔的增加可能会导致真实的预测值对结果的影响变的非常小，而无法非常好的进行预测（RNN中的长期依赖问题（long-Term Dependencies））</p>
<p>那么为了解决这个问题需要<strong>LSTM</strong>（<strong>Long Short-Term Memory网络</strong>）</p>
<p>LSTM是一种RNN特殊的类型，可以学习长期依赖信息。在很多问题上，LSTM都取得相当巨大的成功，并得到了广泛的应用。</p>
<p>一个LSMT的单元就是下图中的一个绿色方框中的内容：</p>
<p><img src="/2021/07/20/pytorch5/LSTM1.jpg"></p>
<p>其中$\sigma$表示sigmod函数，其他符号的含义：</p>
<p><img src="/2021/07/20/pytorch5/LSTM2.jpg"></p>
<h3 id="2-2-LSTM的核心"><a href="#2-2-LSTM的核心" class="headerlink" title="2.2 LSTM的核心"></a>2.2 LSTM的核心</h3><p><img src="/2021/07/20/pytorch5/LSTM3.png"></p>
<p>LSTM的核心在于单元（细胞）中的状态，也就是上图中最上面的那根线。</p>
<p>但是如果只有上面那一条线，那么没有办法实现信息的增加或者删除，所以在LSTM是通过一个叫做<code>门</code>的结构实现，门可以选择让信息通过或者不通过。</p>
<p>这个门主要是通过sigmoid和点乘（<code>pointwise multiplication</code>）实现的</p>
<p><img src="/2021/07/20/pytorch5/LSTM4.png"></p>
<p>$sigmoid$的取值范围是在(0,1)之间，如果接近0表示不让任何信息通过，如果接近1表示所有的信息都会通过</p>
<h3 id="2-3-逐步理解LSTM"><a href="#2-3-逐步理解LSTM" class="headerlink" title="2.3 逐步理解LSTM"></a>2.3 逐步理解LSTM</h3><h4 id="2-3-1-遗忘门"><a href="#2-3-1-遗忘门" class="headerlink" title="2.3.1 遗忘门"></a>2.3.1 遗忘门</h4><p>遗忘门通过sigmoid函数来决定哪些信息会被遗忘</p>
<p>在下图就是$h_{t-1}和x_t$进行合并（concat）之后乘上权重和偏置，通过sigmoid函数，输出0-1之间的一个值，这个值会和前一次的细胞状态($C_{t-1}​$)进行点乘，从而决定遗忘或者保留</p>
<p><img src="/2021/07/20/pytorch5/%E6%98%93%E7%8E%8B%E9%97%A8.png"></p>
<h4 id="2-3-2-输入门"><a href="#2-3-2-输入门" class="headerlink" title="2.3.2 输入门"></a>2.3.2 输入门</h4><p><img src="/2021/07/20/pytorch5/%E8%BE%93%E5%85%A5%E9%97%A8.png"></p>
<p>下一步就是决定哪些新的信息会被保留，这个过程有两步：</p>
<ol>
<li>一个被称为<code>输入门</code>的sigmoid 层决定哪些信息会被更新</li>
<li><code>tanh</code>会创造一个新的候选向量$\widetilde{C}_{t}$，后续可能会被添加到细胞状态中</li>
</ol>
<p>例如：</p>
<p><code>我昨天吃了苹果，今天我想吃菠萝</code>，在这个句子中，通过遗忘门可以遗忘<code>苹果</code>,同时更新新的主语为<code>菠萝</code></p>
<p>现在就可以更新旧的细胞状态$C_{t-1}$为新的$C_{ t }​$ 了。</p>
<p>更新的构成很简单就是：</p>
<ol>
<li>旧的细胞状态和遗忘门的结果相乘</li>
<li>然后加上 输入门和tanh相乘的结果</li>
</ol>
<p><img src="/2021/07/20/pytorch5/LSTM-update.png"></p>
<h4 id="2-3-3-输出门"><a href="#2-3-3-输出门" class="headerlink" title="2.3.3 输出门"></a>2.3.3 输出门</h4><p>最后，我们需要决定什么信息会被输出，也是一样这个输出经过变换之后会通过sigmoid函数的结果来决定那些细胞状态会被输出。</p>
<p><img src="/2021/07/20/pytorch5/%E8%BE%93%E5%87%BA%E9%97%A8.png"></p>
<p>步骤如下：</p>
<ol>
<li>前一次的输出和当前时间步的输入的组合结果通过sigmoid函数进行处理得到$O_t$</li>
<li>更新后的细胞状态$C_t$会经过tanh层的处理，把数据转化到(-1,1)的区间</li>
<li>tanh处理后的结果和$O_t$进行相乘，把结果输出同时传到下一个LSTM的单元</li>
</ol>
<h3 id="2-4-GRU，LSTM的变形"><a href="#2-4-GRU，LSTM的变形" class="headerlink" title="2.4 GRU，LSTM的变形"></a>2.4 GRU，LSTM的变形</h3><p>GRU(Gated Recurrent Unit),是一种LSTM的变形版本， 它将遗忘和输入门组合成一个“更新门”。它还合并了单元状态和隐藏状态，并进行了一些其他更改，由于他的模型比标准LSTM模型简单，所以越来越受欢迎。</p>
<p><img src="/2021/07/20/pytorch5/GRU.png"></p>
<p>LSTM内容参考地址：<a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<h2 id="3-双向LSTM"><a href="#3-双向LSTM" class="headerlink" title="3. 双向LSTM"></a>3. 双向LSTM</h2><p>单向的 RNN，是根据前面的信息推出后面的，但有时候只看前面的词是不够的， 可能需要预测的词语和后面的内容也相关，那么此时需要一种机制，能够让模型不仅能够从前往后的具有记忆，还需要从后往前需要记忆。此时双向LSTM就可以帮助我们解决这个问题</p>
<p><img src="/2021/07/20/pytorch5/bidir_lstm.png"></p>
<p>由于是双向LSTM，所以每个方向的LSTM都会有一个输出，最终的输出会有2部分，所以往往需要concat的操作</p>
<h1 id="循环神经网络在pytorch中的应用"><a href="#循环神经网络在pytorch中的应用" class="headerlink" title="循环神经网络在pytorch中的应用"></a>循环神经网络在pytorch中的应用</h1><h2 id="1-Pytorch中LSTM和GRU模块使用"><a href="#1-Pytorch中LSTM和GRU模块使用" class="headerlink" title="1. Pytorch中LSTM和GRU模块使用"></a>1. Pytorch中LSTM和GRU模块使用</h2><h3 id="1-1-LSTM介绍"><a href="#1-1-LSTM介绍" class="headerlink" title="1.1 LSTM介绍"></a>1.1 LSTM介绍</h3><p>LSTM和GRU都是由<code>torch.nn</code>提供</p>
<p>通过观察文档，可知LSMT的参数，</p>
<p><code>torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first,dropout,bidirectional)</code></p>
<ol>
<li><code>input_size </code>：输入数据的形状，即embedding_dim</li>
<li><code>hidden_size</code>：隐藏层神经元的数量，即每一层有多少个LSTM单元</li>
<li><code>num_layer</code> ：即RNN的中LSTM单元的层数</li>
<li><code>batch_first</code>：默认值为False，输入的数据需要<code>[seq_len,batch,feature]</code>,如果为True，则为<code>[batch,seq_len,feature]</code></li>
<li><code>dropout</code>:dropout的比例，默认值为0。dropout是一种训练过程中让部分参数随机失活的一种方式，能够提高训练速度，同时能够解决过拟合的问题。这里是在LSTM的最后一层，对每个输出进行dropout</li>
<li><code>bidirectional</code>：是否使用双向LSTM,默认是False</li>
</ol>
<p>实例化LSTM对象之后,<strong>不仅需要传入数据，还需要前一次的h_0(前一次的隐藏状态)和c_0（前一次memory）</strong></p>
<p>即：<code>lstm(input,(h_0,c_0))</code></p>
<p>LSTM的默认输出为<code>output, (h_n, c_n)</code></p>
<ol>
<li><code>output</code>：<code>(seq_len, batch, num_directions * hidden_size)</code>—&gt;batch_first=False</li>
<li><code>h_n</code>:<code>(num_layers * num_directions, batch, hidden_size)</code></li>
<li><code>c_n</code>: <code>(num_layers * num_directions, batch, hidden_size)</code></li>
</ol>
<h2 id="1-2-LSTM使用示例"><a href="#1-2-LSTM使用示例" class="headerlink" title="1.2 LSTM使用示例"></a>1.2 LSTM使用示例</h2><p>假设数据输入为 input ,形状是<code>[10,20]</code>，假设embedding的形状是<code>[100,30]</code></p>
<p>则LSTM使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">batch_size =<span class="number">10</span></span><br><span class="line">seq_len = <span class="number">20</span></span><br><span class="line">embedding_dim = <span class="number">30</span></span><br><span class="line">word_vocab = <span class="number">100</span></span><br><span class="line">hidden_size = <span class="number">18</span></span><br><span class="line">num_layer = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#准备输入数据</span></span><br><span class="line"><span class="built_in">input</span> = torch.randint(low=<span class="number">0</span>,high=<span class="number">100</span>,size=(batch_size,seq_len))</span><br><span class="line"><span class="comment">#准备embedding</span></span><br><span class="line">embedding  = torch.nn.Embedding(word_vocab,embedding_dim)</span><br><span class="line">lstm = torch.nn.LSTM(embedding_dim,hidden_size,num_layer)</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行mebed操作</span></span><br><span class="line">embed = embedding(<span class="built_in">input</span>) <span class="comment">#[10,20,30]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#转化数据为batch_first=False</span></span><br><span class="line">embed = embed.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>) <span class="comment">#[20,10,30]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化状态， 如果不初始化，torch默认初始值为全0</span></span><br><span class="line">h_0 = torch.rand(num_layer,batch_size,hidden_size)</span><br><span class="line">c_0 = torch.rand(num_layer,batch_size,hidden_size)</span><br><span class="line">output,(h_1,c_1) = lstm(embed,(h_0,c_0))</span><br><span class="line"><span class="comment">#output [20,10,1*18]</span></span><br><span class="line"><span class="comment">#h_1 [2,10,18]</span></span><br><span class="line"><span class="comment">#c_1 [2,10,18]</span></span><br></pre></td></tr></table></figure>

<p>输出如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">122</span>]: output.size()</span><br><span class="line">Out[<span class="number">122</span>]: torch.Size([<span class="number">20</span>, <span class="number">10</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">123</span>]: h_1.size()</span><br><span class="line">Out[<span class="number">123</span>]: torch.Size([<span class="number">2</span>, <span class="number">10</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">124</span>]: c_1.size()</span><br><span class="line">Out[<span class="number">124</span>]: torch.Size([<span class="number">2</span>, <span class="number">10</span>, <span class="number">18</span>])</span><br></pre></td></tr></table></figure>

<p>通过前面的学习，我们知道，最后一次的h_1应该和output的最后一个time step的输出是一样的</p>
<p>通过下面的代码，我们来验证一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">179</span>]: a = output[-<span class="number">1</span>,:,:]</span><br><span class="line"></span><br><span class="line">In [<span class="number">180</span>]: a.size()</span><br><span class="line">Out[<span class="number">180</span>]: torch.Size([<span class="number">10</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">183</span>]: b.size()</span><br><span class="line">Out[<span class="number">183</span>]: torch.Size([<span class="number">10</span>, <span class="number">18</span>])</span><br><span class="line">In [<span class="number">184</span>]: a == b</span><br><span class="line">Out[<span class="number">184</span>]:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">       dtype=torch.uint8)</span><br></pre></td></tr></table></figure>

<h3 id="1-3-GRU的使用示例"><a href="#1-3-GRU的使用示例" class="headerlink" title="1.3 GRU的使用示例"></a>1.3 GRU的使用示例</h3><p>GRU模块<code>torch.nn.GRU</code>，和LSTM的参数相同，含义相同，具体可参考文档</p>
<p>但是输入只剩下<code>gru(input,h_0)</code>，输出为<code>output, h_n</code></p>
<p>其形状为：</p>
<ol>
<li><code>output</code>:<code>(seq_len, batch, num_directions * hidden_size)</code></li>
<li><code>h_n</code>:<code>(num_layers * num_directions, batch, hidden_size)</code></li>
</ol>
<p>大家可以使用上述代码，观察GRU的输出形式</p>
<h3 id="1-4-双向LSTM"><a href="#1-4-双向LSTM" class="headerlink" title="1.4 双向LSTM"></a>1.4 双向LSTM</h3><p>如果需要使用双向LSTM，则在实例化LSTM的过程中，需要把LSTM中的bidriectional设置为True，同时h_0和c_0使用num_layer*2</p>
<p>观察效果，输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">batch_size =<span class="number">10</span> <span class="comment">#句子的数量</span></span><br><span class="line">seq_len = <span class="number">20</span>  <span class="comment">#每个句子的长度</span></span><br><span class="line">embedding_dim = <span class="number">30</span>  <span class="comment">#每个词语使用多长的向量表示</span></span><br><span class="line">word_vocab = <span class="number">100</span>  <span class="comment">#词典中词语的总数</span></span><br><span class="line">hidden_size = <span class="number">18</span>  <span class="comment">#隐层中lstm的个数</span></span><br><span class="line">num_layer = <span class="number">2</span>  <span class="comment">#多少个隐藏层</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.randint(low=<span class="number">0</span>,high=<span class="number">100</span>,size=(batch_size,seq_len))</span><br><span class="line">embedding  = torch.nn.Embedding(word_vocab,embedding_dim)</span><br><span class="line">lstm = torch.nn.LSTM(embedding_dim,hidden_size,num_layer,bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">embed = embedding(<span class="built_in">input</span>) <span class="comment">#[10,20,30]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#转化数据为batch_first=False</span></span><br><span class="line">embed = embed.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>) <span class="comment">#[20,10,30]</span></span><br><span class="line">h_0 = torch.rand(num_layer*<span class="number">2</span>,batch_size,hidden_size)</span><br><span class="line">c_0 = torch.rand(num_layer*<span class="number">2</span>,batch_size,hidden_size)</span><br><span class="line">output,(h_1,c_1) = lstm(embed,(h_0,c_0))</span><br><span class="line"></span><br><span class="line">In [<span class="number">135</span>]: output.size()</span><br><span class="line">Out[<span class="number">135</span>]: torch.Size([<span class="number">20</span>, <span class="number">10</span>, <span class="number">36</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">136</span>]: h_1.size()</span><br><span class="line">Out[<span class="number">136</span>]: torch.Size([<span class="number">4</span>, <span class="number">10</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">137</span>]: c_1.size()</span><br><span class="line">Out[<span class="number">137</span>]: torch.Size([<span class="number">4</span>, <span class="number">10</span>, <span class="number">18</span>])</span><br></pre></td></tr></table></figure>

<p>在单向LSTM中，最后一个time step的输出的前hidden_size个和最后一层隐藏状态h_1的输出相同，那么双向LSTM呢？</p>
<p>双向LSTM中：</p>
<p><strong>output：按照正反计算的结果顺序在第2个维度进行拼接，正向第一个拼接反向的最后一个输出</strong></p>
<p><strong>hidden state:按照得到的结果在第0个维度进行拼接，正向第一个之后接着是反向第一个</strong></p>
<ol>
<li><p>前向的LSTM中，最后一个time step的输出的前hidden_size个和最后一层向前传播h_1的输出相同</p>
<ul>
<li>示例：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-1是前向LSTM的最后一个，前18是前hidden_size个</span></span><br><span class="line">In [<span class="number">188</span>]: a = output[-<span class="number">1</span>,:,:<span class="number">18</span>]  <span class="comment">#前项LSTM中最后一个time step的output</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">189</span>]: b = h_1[-<span class="number">2</span>,:,:]  <span class="comment">#倒数第二个为前向</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">190</span>]: a.size()</span><br><span class="line">Out[<span class="number">190</span>]: torch.Size([<span class="number">10</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">191</span>]: b.size()</span><br><span class="line">Out[<span class="number">191</span>]: torch.Size([<span class="number">10</span>, <span class="number">18</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">192</span>]: a == b</span><br><span class="line">Out[<span class="number">192</span>]:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">       dtype=torch.uint8)</span><br></pre></td></tr></table></figure>

</li>
<li><p>后向LSTM中，最后一个time step的输出的后hidden_size个和最后一层后向传播的h_1的输出相同</p>
<ul>
<li><p>示例</p>
</li>
<li><pre><code class="python">#0 是反向LSTM的最后一个，后18是后hidden_size个
In [196]: c = output[0,:,18:]  #后向LSTM中的最后一个输出

In [197]: d = h_1[-1,:,:] #后向LSTM中的最后一个隐藏层状态

In [198]: c == d
Out[198]:
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
       dtype=torch.uint8)
</code></pre>
</li>
</ul>
</li>
</ol>
<h3 id="1-5-LSTM和GRU的使用注意点"><a href="#1-5-LSTM和GRU的使用注意点" class="headerlink" title="1.5 LSTM和GRU的使用注意点"></a>1.5 LSTM和GRU的使用注意点</h3><ol>
<li>第一次调用之前，需要初始化隐藏状态，如果不初始化，默认创建全为0的隐藏状态</li>
<li>往往会使用LSTM or GRU 的输出的最后一维的结果，来代表LSTM、GRU对文本处理的结果，其形状为<code>[batch,  num_directions*hidden_size]</code>。<ol>
<li>并不是所有模型都会使用最后一维的结果</li>
<li>如果实例化LSTM的过程中，batch_first=False,则<code>output[-1] or output[-1,:,:]</code>可以获取最后一维</li>
<li>如果实例化LSTM的过程中，batch_first=True,则<code>output[:,-1,:]</code>可以获取最后一维</li>
</ol>
</li>
<li>如果结果是<code>(seq_len, batch_size, num_directions * hidden_size)</code>,需要把它转化为<code>(batch_size,seq_len, num_directions * hidden_size)</code>的形状，不能够不是view等变形的方法，需要使用<code>output.permute(1,0,2)</code>，即交换0和1轴，实现上述效果</li>
<li>使用双向LSTM的时候，往往会分别使用每个方向最后一次的output，作为当前数据经过双向LSTM的结果<ul>
<li>即：<code>torch.cat([h_1[-2,:,:],h_1[-1,:,:]],dim=-1)</code></li>
<li>最后的表示的size是<code>[batch_size,hidden_size*2]</code></li>
</ul>
</li>
<li>上述内容在GRU中同理</li>
</ol>

    </div>

    
    
    
<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------以上<i class="fa fa-paw"></i>（或许会有不定时更新）-------------</div>
    
</div>

  
</div>
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Komeiji Satori
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://kome1jisatori.github.io/2021/07/20/pytorch5/" title="循环神经网络（RNN）原理和简单应用">https://kome1jisatori.github.io/2021/07/20/pytorch5/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/18/pytorch4/" rel="prev" title="Minist手写数字识别">
      <i class="fa fa-chevron-left"></i> Minist手写数字识别
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/08/05/rnn/" rel="next" title="记录下没有训练出成果的情感识别模型">
      记录下没有训练出成果的情感识别模型 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86"><span class="nav-text">循环神经网络原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8B%E7%BB%8D"><span class="nav-text">1. 循环神经网络的介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-LSTM%E5%92%8CGRU"><span class="nav-text">2. LSTM和GRU</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-LSTM%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D"><span class="nav-text">2.1 LSTM的基础介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-LSTM%E7%9A%84%E6%A0%B8%E5%BF%83"><span class="nav-text">2.2 LSTM的核心</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E9%80%90%E6%AD%A5%E7%90%86%E8%A7%A3LSTM"><span class="nav-text">2.3 逐步理解LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-%E9%81%97%E5%BF%98%E9%97%A8"><span class="nav-text">2.3.1 遗忘门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-%E8%BE%93%E5%85%A5%E9%97%A8"><span class="nav-text">2.3.2 输入门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-3-%E8%BE%93%E5%87%BA%E9%97%A8"><span class="nav-text">2.3.3 输出门</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-GRU%EF%BC%8CLSTM%E7%9A%84%E5%8F%98%E5%BD%A2"><span class="nav-text">2.4 GRU，LSTM的变形</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%8F%8C%E5%90%91LSTM"><span class="nav-text">3. 双向LSTM</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9C%A8pytorch%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-text">循环神经网络在pytorch中的应用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Pytorch%E4%B8%ADLSTM%E5%92%8CGRU%E6%A8%A1%E5%9D%97%E4%BD%BF%E7%94%A8"><span class="nav-text">1. Pytorch中LSTM和GRU模块使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-LSTM%E4%BB%8B%E7%BB%8D"><span class="nav-text">1.1 LSTM介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-LSTM%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="nav-text">1.2 LSTM使用示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-GRU%E7%9A%84%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="nav-text">1.3 GRU的使用示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E5%8F%8C%E5%90%91LSTM"><span class="nav-text">1.4 双向LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-LSTM%E5%92%8CGRU%E7%9A%84%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E7%82%B9"><span class="nav-text">1.5 LSTM和GRU的使用注意点</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Komeiji Satori"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Komeiji Satori</p>
  <div class="site-description" itemprop="description">记录些有的没的的学习心得</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">26</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/kome1jisatori" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;kome1jisatori" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=740775405&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;740775405&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      膜大佬的链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://sherlockcoder.xyz/" title="http:&#x2F;&#x2F;sherlockcoder.xyz" rel="noopener" target="_blank">不朽之存在的首页</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://datawhalechina.github.io/leeml-notes/#/" title="https:&#x2F;&#x2F;datawhalechina.github.io&#x2F;leeml-notes&#x2F;#&#x2F;" rel="noopener" target="_blank">机器学习笔记(LeeML-Notes)</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">苏ICP备 - 2021020302 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">KomeijiSatori</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">253k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">3:50</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '8NhM2OcimmX1hgpelhxOhQK8-gzGzoHsz',
      appKey     : 'bQ1303bPFHzcUrkoBzb39dgO',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>



  <script type="text/javascript" color="0,0,0" opacity='0.5' zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"log":false});</script></body>
</html>

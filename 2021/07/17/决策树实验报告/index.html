<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kome1jisatori.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="数据挖掘">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch线性回归">
<meta property="og:url" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/index.html">
<meta property="og:site_name" content="KomeijiSatoriのblog">
<meta property="og:description" content="数据挖掘">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/ID3-1.png">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/ID3-2.png">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.4-1">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.4-2.png">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.4-3.png">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.6-4.png">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.6-5.png">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.6-6.png">
<meta property="og:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.6-5.png">
<meta property="article:published_time" content="2021-07-16T21:10:14.000Z">
<meta property="article:modified_time" content="2022-04-09T09:40:25.750Z">
<meta property="article:author" content="Komeiji Satori">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/ID3-1.png">

<link rel="canonical" href="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>pytorch线性回归 | KomeijiSatoriのblog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">KomeijiSatoriのblog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">地灵殿从零开始的学习生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">33</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Komeiji Satori">
      <meta itemprop="description" content="记录些有的没的的学习心得">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="KomeijiSatoriのblog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          pytorch线性回归
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-17 05:10:14" itemprop="dateCreated datePublished" datetime="2021-07-17T05:10:14+08:00">2021-07-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-09 17:40:25" itemprop="dateModified" datetime="2022-04-09T17:40:25+08:00">2022-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a>
                </span>
            </span>

          
            <span id="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/" class="post-meta-item leancloud_visitors" data-flag-title="pytorch线性回归" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
     
      
        <p>数据挖掘</p>
<span id="more"></span>  

<h1 id="决策树实验报告"><a href="#决策树实验报告" class="headerlink" title="决策树实验报告"></a>决策树实验报告</h1><h2 id="实验要求"><a href="#实验要求" class="headerlink" title="实验要求"></a>实验要求</h2><ol>
<li><p>编程实现一个基于信息熵进行划分选择的决策树算法，并为表中的数据生成一棵决策树。</p>
<table>
<thead>
<tr>
<th>编号</th>
<th>色泽</th>
<th>根蒂</th>
<th>敲声</th>
<th>纹理</th>
<th>脐部</th>
<th>触感</th>
<th>密度</th>
<th>含糖率</th>
<th>好瓜</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>青绿</td>
<td>蜷缩</td>
<td>浊响</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>0.697</td>
<td>0.460</td>
<td>是</td>
</tr>
<tr>
<td>2</td>
<td>乌黑</td>
<td>蜷缩</td>
<td>沉闷</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>0.774</td>
<td>0.376</td>
<td>是</td>
</tr>
<tr>
<td>3</td>
<td>乌黑</td>
<td>蜷缩</td>
<td>浊响</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>0.634</td>
<td>0.264</td>
<td>是</td>
</tr>
<tr>
<td>4</td>
<td>青绿</td>
<td>蜷缩</td>
<td>沉闷</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>0.608</td>
<td>0.318</td>
<td>是</td>
</tr>
<tr>
<td>5</td>
<td>浅白</td>
<td>蜷缩</td>
<td>浊响</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>0.556</td>
<td>0.215</td>
<td>是</td>
</tr>
<tr>
<td>6</td>
<td>青绿</td>
<td>稍蜷</td>
<td>浊响</td>
<td>清晰</td>
<td>稍凹</td>
<td>软粘</td>
<td>0.403</td>
<td>0.237</td>
<td>是</td>
</tr>
<tr>
<td>7</td>
<td>乌黑</td>
<td>稍蜷</td>
<td>浊响</td>
<td>稍糊</td>
<td>稍凹</td>
<td>软粘</td>
<td>0.481</td>
<td>0.149</td>
<td>是</td>
</tr>
<tr>
<td>8</td>
<td>乌黑</td>
<td>稍蜷</td>
<td>浊响</td>
<td>清晰</td>
<td>稍凹</td>
<td>硬滑</td>
<td>0.437</td>
<td>0.211</td>
<td>是</td>
</tr>
<tr>
<td>9</td>
<td>乌黑</td>
<td>稍蜷</td>
<td>沉闷</td>
<td>稍糊</td>
<td>稍凹</td>
<td>硬滑</td>
<td>0.666</td>
<td>0.091</td>
<td>否</td>
</tr>
<tr>
<td>10</td>
<td>青绿</td>
<td>硬挺</td>
<td>清脆</td>
<td>清晰</td>
<td>平坦</td>
<td>软粘</td>
<td>0.243</td>
<td>0.267</td>
<td>否</td>
</tr>
<tr>
<td>11</td>
<td>浅白</td>
<td>硬挺</td>
<td>清脆</td>
<td>模糊</td>
<td>平坦</td>
<td>硬滑</td>
<td>0.245</td>
<td>0.057</td>
<td>否</td>
</tr>
<tr>
<td>12</td>
<td>浅白</td>
<td>蜷缩</td>
<td>浊响</td>
<td>模糊</td>
<td>平坦</td>
<td>软粘</td>
<td>0.343</td>
<td>0.099</td>
<td>否</td>
</tr>
<tr>
<td>13</td>
<td>青绿</td>
<td>稍蜷</td>
<td>浊响</td>
<td>稍糊</td>
<td>凹陷</td>
<td>硬滑</td>
<td>0.639</td>
<td>0.161</td>
<td>否</td>
</tr>
<tr>
<td>14</td>
<td>浅白</td>
<td>稍蜷</td>
<td>沉闷</td>
<td>稍糊</td>
<td>凹陷</td>
<td>硬滑</td>
<td>0.657</td>
<td>0.198</td>
<td>否</td>
</tr>
<tr>
<td>15</td>
<td>乌黑</td>
<td>稍蜷</td>
<td>浊响</td>
<td>清晰</td>
<td>稍凹</td>
<td>软粘</td>
<td>0.360</td>
<td>0.370</td>
<td>否</td>
</tr>
<tr>
<td>16</td>
<td>浅白</td>
<td>蜷缩</td>
<td>浊响</td>
<td>模糊</td>
<td>平坦</td>
<td>硬滑</td>
<td>0.593</td>
<td>0.042</td>
<td>否</td>
</tr>
<tr>
<td>17</td>
<td>青绿</td>
<td>蜷缩</td>
<td>沉闷</td>
<td>稍糊</td>
<td>稍凹</td>
<td>硬滑</td>
<td>0.719</td>
<td>0.103</td>
<td>否</td>
</tr>
</tbody></table>
</li>
<li><p>编程实现基于基尼指数进行划分选择的决策树算法，为表中数据生成预剪枝，后剪枝决策树，并与未剪枝的决策树进行比较。</p>
<table>
<thead>
<tr>
<th>编号</th>
<th>色泽</th>
<th>根蒂</th>
<th>敲声</th>
<th>纹理</th>
<th>脐部</th>
<th>触感</th>
<th>好瓜</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>青绿</td>
<td>蜷缩</td>
<td>浊响</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>是</td>
</tr>
<tr>
<td>2</td>
<td>乌黑</td>
<td>蜷缩</td>
<td>沉闷</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>是</td>
</tr>
<tr>
<td>3</td>
<td>乌黑</td>
<td>蜷缩</td>
<td>浊响</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>是</td>
</tr>
<tr>
<td>4</td>
<td>青绿</td>
<td>蜷缩</td>
<td>沉闷</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>是</td>
</tr>
<tr>
<td>5</td>
<td>浅白</td>
<td>蜷缩</td>
<td>浊响</td>
<td>清晰</td>
<td>凹陷</td>
<td>硬滑</td>
<td>是</td>
</tr>
<tr>
<td>6</td>
<td>青绿</td>
<td>稍蜷</td>
<td>浊响</td>
<td>清晰</td>
<td>稍凹</td>
<td>软粘</td>
<td>是</td>
</tr>
<tr>
<td>7</td>
<td>乌黑</td>
<td>稍蜷</td>
<td>浊响</td>
<td>稍糊</td>
<td>稍凹</td>
<td>软粘</td>
<td>是</td>
</tr>
<tr>
<td>8</td>
<td>乌黑</td>
<td>稍蜷</td>
<td>浊响</td>
<td>清晰</td>
<td>稍凹</td>
<td>硬滑</td>
<td>是</td>
</tr>
<tr>
<td>9</td>
<td>乌黑</td>
<td>稍蜷</td>
<td>沉闷</td>
<td>稍糊</td>
<td>稍凹</td>
<td>硬滑</td>
<td>否</td>
</tr>
<tr>
<td>10</td>
<td>青绿</td>
<td>硬挺</td>
<td>清脆</td>
<td>清晰</td>
<td>平坦</td>
<td>软粘</td>
<td>否</td>
</tr>
<tr>
<td>11</td>
<td>浅白</td>
<td>硬挺</td>
<td>清脆</td>
<td>模糊</td>
<td>平坦</td>
<td>硬滑</td>
<td>否</td>
</tr>
<tr>
<td>12</td>
<td>浅白</td>
<td>蜷缩</td>
<td>浊响</td>
<td>模糊</td>
<td>平坦</td>
<td>软粘</td>
<td>否</td>
</tr>
<tr>
<td>13</td>
<td>青绿</td>
<td>稍蜷</td>
<td>浊响</td>
<td>稍糊</td>
<td>凹陷</td>
<td>硬滑</td>
<td>否</td>
</tr>
<tr>
<td>14</td>
<td>浅白</td>
<td>稍蜷</td>
<td>沉闷</td>
<td>稍糊</td>
<td>凹陷</td>
<td>硬滑</td>
<td>否</td>
</tr>
<tr>
<td>15</td>
<td>乌黑</td>
<td>稍蜷</td>
<td>浊响</td>
<td>清晰</td>
<td>稍凹</td>
<td>软粘</td>
<td>否</td>
</tr>
<tr>
<td>16</td>
<td>浅白</td>
<td>蜷缩</td>
<td>浊响</td>
<td>模糊</td>
<td>平坦</td>
<td>硬滑</td>
<td>否</td>
</tr>
<tr>
<td>17</td>
<td>青绿</td>
<td>蜷缩</td>
<td>沉闷</td>
<td>稍糊</td>
<td>稍凹</td>
<td>硬滑</td>
<td>否</td>
</tr>
</tbody></table>
</li>
<li><p>选择两个UCI数据集，对上述3种算法产生的未剪枝，预剪枝，后剪枝的决策树进行实验比较</p>
</li>
</ol>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><ul>
<li>定义：一种描述对实例进行分类的树形结构。决策树由点和有向边组成。节点有两种类型：内部节点和叶节点。内部节点表示一种特征或者属性，叶节点表示一个分类。构建决策树时通常采用自上而下的方法，在每一步选择一个最好的属性来分裂。[8] “最好” 的定义是使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义”最好”。</li>
<li>意义：每次都找不同的切分点，将样本空间逐渐进行细分，最后把属于同一类的空间进行合并，就形成了决策边界，树的层次越深，决策边界的切分就越细，区分越准确，同时也越有可能产生过拟合。<ul>
<li>决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。</li>
<li>分支使用的是节点属性中的离散型数据，如果数据是连续型的，也需要转化成离散型数据才能在决策树中展示。</li>
<li>决策树的路径具有一个重要的性质：互斥且完备,即每一个样本均被且只能被一条路径所覆盖。决策树学习算法主要由三部分构成：<ul>
<li>特征选择</li>
<li>决策树生成</li>
<li>决策树的剪枝</li>
</ul>
</li>
</ul>
</li>
<li>决策树的建立<br>开始，构建根节点，将所有训练数据放在根节点，选择一个最优特征，按照这一特征的取值将训练数据分割为子集，使各个子集有一个当前条件下最好的分类。如果这些子集能被基本正确分类，那么构造叶节点，将对应子集集中到叶节点。如果有子集不能被正确分类，那么就这些子集选择新的最优特征，继续对其进行分割，构建相应的节点。递归进行上述的操作，直到所有训练数据子集均能被正确分类。<ul>
<li>节点分裂：一般当一个节点所代表的属性无法给出判断时，则选择将这一节点分成2个子节点（如不是二叉树的情况会分成n个子节点）</li>
<li>阈值的确定，选择适当的阈值使得分类错误率最小。</li>
</ul>
</li>
<li>与其他的数据挖掘算法相比，决策树有许多优点:<ul>
<li>易于理解和解释，人们很容易理解决策树的意义。</li>
<li>只需很少的数据准备，其他技术往往需要数据归一化。</li>
<li>既可以处理数值型数据也可以处理类别型数据。其他技术往往只能处理一种数据类型。例如关联规则只能处理类别型的而神经网络只能处理数值型的数据。</li>
<li>使用白箱模型，输出结果容易通过模型的结构来解释。而神经网络是黑箱模型，很难解释输出的结果。</li>
<li>可以通过测试集来验证模型的性能，可以考虑模型的稳定性。</li>
<li>强健控制，对噪声处理有好的强健性。</li>
<li>可以很好的处理大规模数据 。</li>
</ul>
</li>
</ul>
<h3 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h3><ul>
<li><p>信息熵 entropy<br>  熵是接收的每条消息中包含的信息的平均量，熵是对不确定性的测量。但是在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。<br>  当取自有限的样本时，熵的公式可以表示为：<br>  $$H(X) = -\sum^{n}_{i = 1}p_i \log_2 p_i$$<br>  其中若 $p_i = 0$ 则定义 $p_i\log p_i = 0$</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_ent</span>(<span class="params">datasets</span>):</span></span><br><span class="line">    data_length = <span class="built_in">len</span>(datasets)</span><br><span class="line">    label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_length):</span><br><span class="line">        label = datasets[i][-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count:</span><br><span class="line">            label_count[label] = <span class="number">0</span></span><br><span class="line">        label_count[label] += <span class="number">1</span></span><br><span class="line">    ent = -<span class="built_in">sum</span>([(p / data_length) * log(p / data_length, <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> label_count.values()])</span><br><span class="line">    <span class="keyword">return</span> ent</span><br></pre></td></tr></table></figure>

<ul>
<li>信息量<br>信息量是对信息的度量，就跟时间的度量是秒一样，当我们考虑一个离散的随机变量x的时候，当我们观察到的这个变量的一个具体值的时候，我们接收到了多少信息呢？多少信息用信息量来衡量，我们接受到的信息量跟具体发生的事件有关。信息的大小跟随机事件的概率有关。越小概率的事情发生了产生的信息量越大，越大概率的事情发生了产生的信息量越小</li>
</ul>
</li>
<li><p>信息增益 information gain<br>  得知特征 $X$ 的信息而使得类 $Y$ 的信息的不确定性减少的程度。<br>  特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$ 定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 的差。<br>  $$g(D,A) = H(D) - H(D|A)$$<br>  一般地，熵 $H(Y)$ 与条件熵 $H(Y|X)$ 之差称为互信息(mutual information) 根据信息增益准则进行特征选择的方法是：对训练数据集D，计算其每个特征的信息增益，并比它们的大小，从而选择信息增益最大的特征。<br>  假设训练数据集为 $D$，样本容量为 $|D|$,有 $K$ 个类别 $C_k$，$|C_k|$ 为类别 $C_k$ 的样本个数。某一特征 $A$ 有 $n$ 个不同的取值 $a_1, a_2, …, a_n$。根据特征 $A$ 的取值可将数据集 $D$ 划分为 $n$ 个子集 $D_1, D_2, …, D_n$， $|D_i$|为 $D_i$ 的样本个数。并记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$，$|D_{ik}|$ 为 $D_{ik}$ 的样本个数。</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cond_ent</span>(<span class="params">datasets, axis=<span class="number">0</span></span>):</span></span><br><span class="line">    data_length = <span class="built_in">len</span>(datasets)</span><br><span class="line">    feature_sets = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_length):</span><br><span class="line">        feature = datasets[i][axis]</span><br><span class="line">        <span class="keyword">if</span> feature <span class="keyword">not</span> <span class="keyword">in</span> feature_sets:</span><br><span class="line">            feature_sets[feature] = []</span><br><span class="line">        feature_sets[feature].append(datasets[i])</span><br><span class="line">    cond_ent = <span class="built_in">sum</span>(</span><br><span class="line">        [(<span class="built_in">len</span>(p) / data_length) * calc_ent(p) <span class="keyword">for</span> p <span class="keyword">in</span> feature_sets.values()])</span><br><span class="line">    <span class="keyword">return</span> cond_ent</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_gain</span>(<span class="params">ent, cond_ent</span>):</span></span><br><span class="line">    <span class="keyword">return</span> ent - cond_ent</span><br></pre></td></tr></table></figure></li>
</ul>
<p>ID3算法的核心是在决策树的各个结点上应用信息增益准则进行特征选择。具体做法是：</p>
<ul>
<li>从根节点开始，对结点计算所有可能特征的信息增益，选择信息增益最大的特征作为结点的特征，并由该特征的不同取值构建子节点；</li>
<li>对子节点递归地调用以上方法，构建决策树；</li>
<li>直到所有特征的信息增益均很小或者没有特征可选时为止。</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">判断数据集中的每个子项是否属于同一类：</span><br><span class="line">    if true:</span><br><span class="line">        return 类标签；</span><br><span class="line">    else:</span><br><span class="line">        寻找划分数据集的最佳特征</span><br><span class="line">        根据最佳特征划分数据集</span><br><span class="line">        创建分支节点</span><br><span class="line">        for 每个划分的子集</span><br><span class="line">            递归调用createBranch();</span><br><span class="line">        return 分支节点</span><br></pre></td></tr></table></figure>

<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><ul>
<li>信息增益比 information gain ratio<br>  以信息增益作为划分训练数据集的特征，存在偏向与选择取值较多的特征的问题。使用信息增益比可以对这个问题进行矫正。<br>  特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 和训练数据集 $D$ 关于特征 $A$ 的熵 $H_A(D)$ 的比值。<br>  $$g_R(D,A) = \frac{g(D,A)}{H_A(D,A)}$$<br>  其中 $H_A(D) = \sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}$, $n$ 是特征 $A$ 取值的个数。</li>
</ul>
<p>相比 ID3 算法，C4.5 算法更换了特征选择的标准，使用信息增益比进行特征选择。不直接选择增益率最大的候选划分属性，候选划分属性中找出信息增益高于平均水平的属性（这样保证了大部分好的的特征），再从中选择增益率最高的（又保证了不会出现编号特征这种极端的情况）<br>对于连续值属性来说，可取值数目不再有限，因此可以采用离散化技术（如二分法）进行处理。将属性值从小到大排序，然后选择中间值作为分割点，数值比它小的点被划分到左子树，数值不小于它的点被分到又子树，计算分割的信息增益率，选择信息增益率最大的属性值进行分割。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Function C4.5(R:包含连续属性的无类别属性集合,C:类别属性,S:训练集)  </span><br><span class="line">/*返回一棵决策树*/  </span><br><span class="line">Begin  </span><br><span class="line">   If S 为空,返回一个值为 Failure 的单个节点;  </span><br><span class="line">   If S 是由相同类别属性值的记录组成：  </span><br><span class="line">      返回一个带有该值的单个节点;  </span><br><span class="line">   If R 为空,则返回一个单节点,其值为在 S 的记录中找出的频率最高的类别属性值;  </span><br><span class="line">   [注意未出现错误则意味着是不适合分类的记录]；  </span><br><span class="line">  For 所有的属性 R(Ri) Do  </span><br><span class="line">        If 属性 Ri 为连续属性，则  </span><br><span class="line">     Begin  </span><br><span class="line">           将Ri的最小值赋给 A1：  </span><br><span class="line">        将Rm的最大值赋给Am；/*m值手工设置*/  </span><br><span class="line">           For j From 2 To m-1 Do Aj=A1+j*(A1Am)/m;  </span><br><span class="line">           将 Ri 点的基于&#123;&lt; =Aj,&gt;Aj&#125;的最大信息增益属性 (Ri,S) 赋给 A；  </span><br><span class="line">     End；  </span><br><span class="line">  将 R 中属性之间具有最大信息增益的属性 (D,S) 赋给 D;  </span><br><span class="line">   将属性D的值赋给&#123;dj/j=1,2...m&#125;；  </span><br><span class="line">  将分别由对应于 D 的值为 dj 的记录组成的S的子集赋给 &#123;sj/j=1,2...m&#125;;  </span><br><span class="line">   返回一棵树，其根标记为 D;树枝标记为 d1,d2...dm;  </span><br><span class="line">   再分别构造以下树:  </span><br><span class="line">   C4.5(R-&#123;D&#125;,C,S1),C4.5(R-&#123;D&#125;,C,S2)...C4.5(R-&#123;D&#125;,C,Sm);  </span><br><span class="line"></span><br><span class="line">End C4.5</span><br></pre></td></tr></table></figure>

<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>CART与ID3区别： CART中用于选择变量的不纯性度量是Gini指数； 如果目标变量是标称的，并且是具有两个以上的类别，则CART可能考虑将目标类别合并成两个超类别（双化）； 如果目标变量是连续的，则CART算法找出一组基于树的回归方程来预测目标变量。</p>
<ul>
<li><p>Gini 指数<br>  分类问题中假设有 $K$ 个类，样本点属于第 $k$ 个类的概率为 $p_k$，则概率分布的基尼指数为定义为<br>  $$Gini(p) = \sum_{k=1}^K p_k(1 - p_k) = 1 - \sum_{k = 1}^K p_k^2$$<br>  对于二分类问题和给定的样本集合 $D$ 其基尼指数为<br>  $$Gini(D) = 1 - \sum_{k=1}^K (\frac{|C_k|}{|D|})^2$$<br>  若样本集合 $D$ 根据特征 $A$ 是否取某一可能的值 $a$ 分割为 $D_1, D_2$ 两部分，则在特征 $A$ 的条件下集合 $D$ 的基尼指数定义为<br>  $$Gini(D,A) = \frac{|D_1|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)$$<br>  $Gini(D)$ 反映了数据集 $D$ 的纯度，值越小，纯度越高。我们在候选集合中选择使得划分后基尼指数最小的属性作为最优化分属性。</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gini</span>(<span class="params">data_set</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算gini的值，即Gini(p)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    length = <span class="built_in">len</span>(data_set)</span><br><span class="line">    category_2_cnt = calculate_diff_count(data_set)</span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> category <span class="keyword">in</span> category_2_cnt:</span><br><span class="line">        <span class="built_in">sum</span> += <span class="built_in">pow</span>(<span class="built_in">float</span>(category_2_cnt[category]) / length, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - <span class="built_in">sum</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>CART是一棵二叉树，采用二元切分法，每次把数据切成两份，分别进入左子树、右子树。而且每个非叶子节点都有两个孩子，所以CART的叶子节点比非叶子多1。相比ID3和C4.5，CART应用要多一些，既可以用于分类也可以用于回归。CART分类时，使用基尼指数（Gini）来选择最好的数据分割的特征，gini描述的是纯度，与信息熵的含义相似。CART中每一次迭代都会降低GINI系数。</p>
<ul>
<li><p>算法流程：</p>
<ol>
<li>CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</li>
<li>选择最优切分变量j与切分点 $s$：遍历变量 $j$，对规定的切分变量 $j$ 扫描切分点 $s$，选择使下式得到最小值时的 $(j,s)$ 对。其中 $R_m$ 是被划分的输入空间，$c_m$ 是空间 $R_m$ 对应的固定输出值。</li>
<li>用选定的 $(j,s)$ 对，划分区域并决定相应的输出值。</li>
<li>继续对两个子区域调用上述步骤，将输入空间划分为 $M$ 个区域 $R_1,R_2,…,R_m$，生成决策树。</li>
</ol>
<p>  当输入空间划分确定时，可以用平方误差来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。<br>  $$\sum_{x_i \in R_m} (y_i - f(x_i))^2$$</p>
</li>
</ul>
<h3 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h3><p>为了避免决策树“过拟合”样本。前面的算法生成的决策树非常的详细而庞大，每个属性都被详细地加以考虑，决策树的树叶节点所覆盖的训练样本都是“纯”的。因此用这个决策树来对训练样本进行分类的话，你会发现对于训练样本而言，这个树表现堪称完美，它可以100%完美正确得对训练样本集中的样本进行分类（因为决策树本身就是100%完美拟合训练样本的产物）。但是，这会带来一个问题，如果训练样本中包含了一些错误，按照前面的算法，这些错误也会100%一点不留得被决策树学习了，这就是“过拟合”。</p>
<h4 id="预剪枝-pre-pruning"><a href="#预剪枝-pre-pruning" class="headerlink" title="预剪枝 pre-pruning"></a>预剪枝 pre-pruning</h4><p>预剪枝就是在树的构建过程（只用到训练集），设置一个阈值（样本个数小于预定阈值或GINI指数小于预定阈值），使得当在当前分裂节点中分裂前和分裂后的误差超过这个阈值则分列，否则不进行分裂操作。所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。但是这种方法实际中的效果并不好。<br>在划分之前，所有样本集中于根节点，若不进行划分，该节点被标记为叶节点，其类别标记为训练样例最多的类别。若进行划分在测试集上的准确率小于在根节点不进行划分的准确率，或增幅没有超过阈值，都不进行划分，作为一个叶节点返回当前数据集中最多的标签类型。</p>
<ul>
<li><p>预剪枝就是在完全正确分类训练集之前，较早地停止树的生长。 具体在什么时候停止决策树的生长有多种不同的方法:</p>
<ol>
<li>一种最为简单的方法就是在决策树到达一定高度的情况下就停止树的生长。</li>
<li>到达此结点的实例具有相同的特征向量，而不必一定属于同一类， 也可停止生长。</li>
<li>到达此结点的实例个数小于某一个阈值也可停止树的生长。</li>
<li>还有一种更为普遍的做法是计算每次扩张对系统性能的增益，如果这个增益值小于某个阈值则不进行扩展。</li>
</ol>
</li>
<li><p>优点：快速，可以在构建决策树时进行剪枝，显著降低了过拟合风险。由于预剪枝不必生成整棵决策树，且算法相对简单，效率很高，适合解决大规模问题。但是尽管这一方法看起来很直接， 但是怎样精确地估计何时停止树的增长是相当困难的。</p>
</li>
<li><p>缺点：预剪枝基于贪心思想，本质上禁止分支展开，给决策树带来了欠拟合的风险。因为视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> pre_pruning:</span><br><span class="line">    ans = []</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_dataset)):  <span class="comment"># build label for test dataset</span></span><br><span class="line">        ans.append(test_dataset[index][-<span class="number">1</span>])</span><br><span class="line">    result_counter = Counter()</span><br><span class="line">    <span class="keyword">for</span> vec <span class="keyword">in</span> dataset:</span><br><span class="line">        result_counter[vec[-<span class="number">1</span>]] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># what will it be if it is a leaf node</span></span><br><span class="line">    leaf_output = result_counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    root_acc = cal_acc(test_output=[leaf_output] * <span class="built_in">len</span>(test_dataset),</span><br><span class="line">                        label=ans)</span><br><span class="line">    outputs = []</span><br><span class="line">    ans = []</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:  <span class="comment"># expand the node</span></span><br><span class="line">        cut_testset = splitdataset(test_dataset, bestFeat, value)</span><br><span class="line">        cut_dataset = splitdataset(dataset, bestFeat, value)</span><br><span class="line">        <span class="keyword">for</span> vec <span class="keyword">in</span> cut_testset:</span><br><span class="line">            ans.append(vec[-<span class="number">1</span>])</span><br><span class="line">        result_counter = Counter()</span><br><span class="line">        <span class="keyword">for</span> vec <span class="keyword">in</span> cut_dataset:</span><br><span class="line">            result_counter[vec[-<span class="number">1</span>]] += <span class="number">1</span></span><br><span class="line">        leaf_output = result_counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># what will it be if it is a leaf node</span></span><br><span class="line">        outputs += [leaf_output] * <span class="built_in">len</span>(cut_testset)</span><br><span class="line">    cut_acc = cal_acc(test_output=outputs, label=ans)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cut_acc &lt;= root_acc + threshold: <span class="comment"># whether expand the node or not</span></span><br><span class="line">        <span class="keyword">return</span> leaf_output</span><br></pre></td></tr></table></figure>

<h4 id="后剪枝-post-pruning"><a href="#后剪枝-post-pruning" class="headerlink" title="后剪枝 post-pruning"></a>后剪枝 post-pruning</h4><p>决策树构造完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。后剪枝是目前最普遍的做法。<br>后剪枝的剪枝过程是删除一些子树，然后用其叶子节点代替，这个叶子节点所标识的类别通过大多数原则 (majority class criterion) 确定。所谓大多数原则，是指剪枝过程中, 将一些子树删除而用叶节点代替,这个叶节点所标识的类别用这棵子树中大多数训练样本所属的类别来标识。相比于前剪枝，后剪枝方法更常用，是因为在前剪枝方法中精确地估计何时停止树增长很困难。</p>
<ul>
<li>优点：欠拟合风险小，泛化性能好</li>
<li>缺点：在生成决策树之后完成，自底向上对所有非叶节点进行逐一考察，训练的时间开销较大</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune_tree</span>(<span class="params">node, prunedList</span>):</span></span><br><span class="line">    <span class="comment"># Base case: we&#x27;ve reached a leaf</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(node, Leaf):</span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line">    <span class="comment"># If we reach a pruned node, make that node a leaf node and return.</span></span><br><span class="line">    <span class="comment"># Since it becomes a leaf node, the nodes</span></span><br><span class="line">    <span class="comment"># below it are automatically not considered</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">int</span>(node.<span class="built_in">id</span>) <span class="keyword">in</span> prunedList:</span><br><span class="line">        <span class="keyword">return</span> Leaf(node.rows, node.<span class="built_in">id</span>, node.depth)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Call this function recursively on the true branch</span></span><br><span class="line">    node.true_branch = prune_tree(node.true_branch, prunedList)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Call this function recursively on the false branch</span></span><br><span class="line">    node.false_branch = prune_tree(node.false_branch, prunedList)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure>

<h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>测试数据使用李航老师的统计方法中对应的数据集。</p>
<h3 id="ID3-算法实现"><a href="#ID3-算法实现" class="headerlink" title="ID3 算法实现"></a>ID3 算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ID3_chooseBestFeature</span>(<span class="params">dataset</span>):</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataset[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    baseEnt = cal_entropy(dataset)</span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeature = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):  <span class="comment"># check all features</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)</span><br><span class="line">        newEnt = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># claculate entropy of every divide ways</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            <span class="comment"># choose the samples mmeeting the requirement</span></span><br><span class="line">            subdataset = splitdataset(dataset, i, value)</span><br><span class="line">            p = <span class="built_in">len</span>(subdataset) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataset))</span><br><span class="line">            newEnt += p * cal_entropy(subdataset)</span><br><span class="line">        infoGain = baseEnt - newEnt</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain  <span class="comment"># choose the largest information gain</span></span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>

<h3 id="C4-5-算法实现"><a href="#C4-5-算法实现" class="headerlink" title="C4.5 算法实现"></a>C4.5 算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">C45_chooseBestFeatureToSplit</span>(<span class="params">dataset</span>):</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataset[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    baseEnt = cal_entropy(dataset)</span><br><span class="line">    bestInfoGain_ratio = <span class="number">0.0</span></span><br><span class="line">    bestFeature = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):  <span class="comment"># check every feature</span></span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)</span><br><span class="line">        newEnt = <span class="number">0.0</span></span><br><span class="line">        IV = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subdataset = splitdataset(dataset, i, value)</span><br><span class="line">            p = <span class="built_in">len</span>(subdataset) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataset))</span><br><span class="line">            newEnt += p * cal_entropy(subdataset)</span><br><span class="line">            IV = IV - p * log(p, <span class="number">2</span>)</span><br><span class="line">        infoGain = baseEnt - newEnt</span><br><span class="line">        <span class="keyword">if</span> (IV == <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        infoGain_ratio = infoGain / IV  <span class="comment"># infoGain_ratio of current feature</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (infoGain_ratio &gt; bestInfoGain_ratio):  <span class="comment"># choose the greatest gain ratio</span></span><br><span class="line">            bestInfoGain_ratio = infoGain_ratio</span><br><span class="line">            bestFeature = i  <span class="comment"># choose the feature corsbounding to the gain ratio</span></span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>

<h3 id="CART-算法实现"><a href="#CART-算法实现" class="headerlink" title="CART 算法实现"></a>CART 算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CART_chooseBestFeature</span>(<span class="params">dataset</span>):</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataset[<span class="number">0</span>]) - <span class="number">1</span>  <span class="comment"># except the column of labels</span></span><br><span class="line">    bestGini = <span class="number">999999.0</span></span><br><span class="line">    bestFeature = -<span class="number">1</span>  <span class="comment"># default label</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):</span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)  <span class="comment"># get the possible values of each feature</span></span><br><span class="line">        gini = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subdataset = splitdataset(dataset, i, value)</span><br><span class="line">            p = <span class="built_in">len</span>(subdataset) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataset))</span><br><span class="line">            subp = <span class="built_in">len</span>(splitdataset(subdataset, -<span class="number">1</span>, <span class="string">&#x27;0&#x27;</span>)) / <span class="built_in">float</span>(<span class="built_in">len</span>(subdataset))</span><br><span class="line">        gini += p * (<span class="number">1.0</span> - <span class="built_in">pow</span>(subp, <span class="number">2</span>) - <span class="built_in">pow</span>(<span class="number">1</span> - subp, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (gini &lt; bestGini):</span><br><span class="line">            bestGini = gini</span><br><span class="line">            bestFeature = i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>

<h3 id="建树操作"><a href="#建树操作" class="headerlink" title="建树操作"></a>建树操作</h3><p>因为建树过程相似，仅选取 ID3 算法的建树过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ID3_create_tree</span>(<span class="params">dataset, labels, test_dataset</span>):</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):</span><br><span class="line">        <span class="comment"># 类别完全相同，停止划分</span></span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataset[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 遍历完所有特征时返回出现次数最多的</span></span><br><span class="line">        <span class="keyword">return</span> majority_count(classList)</span><br><span class="line">    bestFeat = ID3_choose_best_feature(dataset)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">u&quot;此时最优索引为：&quot;</span> + (bestFeatLabel))</span><br><span class="line"></span><br><span class="line">    ID3Tree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span> (labels[bestFeat])</span><br><span class="line">    <span class="comment"># 得到列表包括节点所有的属性值</span></span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">    uniqueVals = <span class="built_in">set</span>(featValues)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:  <span class="comment"># 枚举对用特征的每个取值</span></span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        ID3Tree[bestFeatLabel][value] = ID3_create_tree(</span><br><span class="line">            split_dataset(dataset, bestFeat, value),</span><br><span class="line">            subLabels,</span><br><span class="line">            split_dataset(test_dataset, bestFeat, value))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cut_acc &gt;= root_acc:</span><br><span class="line">            <span class="keyword">return</span> leaf_output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ID3Tree  <span class="comment"># 如果没有剪枝返回节点</span></span><br></pre></td></tr></table></figure>

<h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>在实现时检查划分，如果在测试集上的准确率下降或没有上升到一个阈值时，将进行剪枝。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> pre_pruning:</span><br><span class="line">    ans = []</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_dataset)):</span><br><span class="line">        ans.append(test_dataset[index][-<span class="number">1</span>])</span><br><span class="line">    result_counter = Counter()</span><br><span class="line">    <span class="keyword">for</span> vec <span class="keyword">in</span> dataset:</span><br><span class="line">        result_counter[vec[-<span class="number">1</span>]] += <span class="number">1</span></span><br><span class="line">    leaf_output = result_counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    root_acc = cal_acc(test_output=[leaf_output] * <span class="built_in">len</span>(test_dataset), label=ans)</span><br><span class="line">    <span class="comment"># 若当前节点是叶节点的准确率</span></span><br><span class="line">    outputs = []</span><br><span class="line">    ans = []</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        cut_testset = split_dataset(test_dataset, bestFeat, value)</span><br><span class="line">        cut_dataset = split_dataset(dataset, bestFeat, value)</span><br><span class="line">        <span class="keyword">for</span> vec <span class="keyword">in</span> cut_testset:</span><br><span class="line">            ans.append(vec[-<span class="number">1</span>])</span><br><span class="line">        result_counter = Counter()</span><br><span class="line">        <span class="keyword">for</span> vec <span class="keyword">in</span> cut_dataset:</span><br><span class="line">            result_counter[vec[-<span class="number">1</span>]] += <span class="number">1</span></span><br><span class="line">        leaf_output = result_counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        outputs += [leaf_output] * <span class="built_in">len</span>(cut_testset)</span><br><span class="line">    cut_acc = cal_acc(test_output=outputs, label=ans)  <span class="comment"># 不进行剪枝在测试集上的准确率</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cut_acc &lt;= root_acc + threshold:  <span class="comment"># 检查准确率上升情况</span></span><br><span class="line">        <span class="keyword">return</span> leaf_output</span><br></pre></td></tr></table></figure>

<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>因为后剪枝的方法是自下而上的判断是否应该进行剪枝，所以在实现时在返回节点对象之前进行剪枝，决定是返回对象还是返回单一类别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> post_pruning:</span><br><span class="line">    tree_output = test_tree(C45Tree,</span><br><span class="line">                            featLabels=total_labels,</span><br><span class="line">                            testDataSet=test_dataset)</span><br><span class="line">    ans = []</span><br><span class="line">    <span class="keyword">for</span> vec <span class="keyword">in</span> test_dataset:</span><br><span class="line">        ans.append(vec[-<span class="number">1</span>])</span><br><span class="line">    root_acc = cal_acc(tree_output, ans)</span><br><span class="line">    result_counter = Counter()</span><br><span class="line">    <span class="keyword">for</span> vec <span class="keyword">in</span> dataset:</span><br><span class="line">        result_counter[vec[-<span class="number">1</span>]] += <span class="number">1</span></span><br><span class="line">    leaf_output = result_counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    cut_acc = cal_acc([leaf_output] * <span class="built_in">len</span>(test_dataset), ans)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cut_acc &gt;= root_acc:</span><br><span class="line">        <span class="keyword">return</span> leaf_output</span><br></pre></td></tr></table></figure>

<p>表面上后剪枝的操作比预剪枝少，实际上在测试时递归的测试了当前节点所在子树的正确率。所以后剪枝带来的开销远大于的预剪枝。</p>
<h2 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h2><h3 id="西瓜数据集3-0"><a href="#西瓜数据集3-0" class="headerlink" title="西瓜数据集3.0"></a>西瓜数据集3.0</h3><p>实现基于信息熵进行划分的决策树算法（ID3）算法，并可视化结果如下：</p>
<p><img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/ID3-1.png" alt="img"></p>
<p>可以注意到直接使用了编号信息进行划分，这一点和书中描述的 “基于信息增益的算法对可取数值较多的属性有偏好” 一致。下来除去编号属性并对结果进行可视化，结果如下：</p>
<p><img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/ID3-2.png" alt="img"></p>
<h3 id="西瓜数据集2-0"><a href="#西瓜数据集2-0" class="headerlink" title="西瓜数据集2.0"></a>西瓜数据集2.0</h3><p>实现基于基尼指数进行划分选择的决策树算法，并进行相应的剪枝操作，使用书中的测试集，验证集划分。</p>
<ul>
<li>未剪枝  <img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.4-1'.png" height="400"></li>
<li>预剪枝  <img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.4-2.png" height="150"></li>
<li>后剪枝  <img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.4-3.png" height="300"></li>
</ul>
<h3 id="UCI-数据集"><a href="#UCI-数据集" class="headerlink" title="UCI 数据集"></a>UCI 数据集</h3><p>使用 UCI 数据集来判断未剪枝，预剪枝，后剪枝的方法产生的差异。</p>
<h4 id="Iris"><a href="#Iris" class="headerlink" title="Iris"></a>Iris</h4><p>总数据集一共 150 条数据，共三个类别。在数据集中随机抽取30条数据组成测试集，再从中随机抽取30个作为训练时的验证集，余下的90条数据组成训练集。数据集的描述内容包括：花萼长度，花萼宽度，花瓣长度，花瓣宽度。</p>
<ul>
<li>使用 CART 进行实验<ul>
<li>未剪枝<pre><code>&lt;img src=&quot;4.6-1.png&quot; height=500&gt;
</code></pre>
</li>
<li>预剪枝<pre><code>&lt;img src=&quot;4.6-2.png&quot; height=400&gt;
</code></pre>
</li>
<li>后剪枝<pre><code> 不是剪枝一定要剪掉某个节点，如果树中本身的节点符合要求，可以不用剪枝。
&lt;img src=&quot;4.6-1.png&quot; height=500&gt;
</code></pre>
</li>
</ul>
</li>
</ul>
<p>经过观察，通过修改随机数种子来更改测试集和验证集的划分可以很大程度上影响决策树的准确率。</p>
<p>将数据集划分函数的 <code>random state</code> 从 15 改成 1 之后，后剪枝产生的决策树变为下图：<br><img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.6-4.png" alt="img"><br>所以数据集的划分可以显著影响树的形状和结果的准确率。</p>
<h4 id="Balloons"><a href="#Balloons" class="headerlink" title="Balloons"></a>Balloons</h4><ul>
<li><p>数据集描述<br>  数据集较小，共20个样本，四个属性。<a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Balloons">数据集链接</a></p>
<table>
<thead>
<tr>
<th>列名</th>
<th>取值</th>
</tr>
</thead>
<tbody><tr>
<td>color</td>
<td>yellow, purple</td>
</tr>
<tr>
<td>size</td>
<td>large, small</td>
</tr>
<tr>
<td>act</td>
<td>stretch, dip</td>
</tr>
<tr>
<td>age</td>
<td>adult, child</td>
</tr>
<tr>
<td>label</td>
<td>T, F</td>
</tr>
</tbody></table>
<p>  其中选择4个样本作为测试集，4个样本作为训练时使用的验证集，余下的作为训练集。</p>
</li>
<li><p>不剪枝<br>  <img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.6-5.png" alt="img"></p>
</li>
<li><p>预剪枝<br>  <img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.6-6.png" alt="img"></p>
</li>
<li><p>后剪枝<br>  <img src="/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/4.6-5.png" alt="img"></p>
</li>
</ul>
<h2 id="个人感悟"><a href="#个人感悟" class="headerlink" title="个人感悟"></a>个人感悟</h2><p>通过本次实验有如下的发现和收获：</p>
<ul>
<li>训练数据集大小和模型精度的关系：<ul>
<li>当训练数据集过小时，建立的模型精度过低，不具有参考价值。</li>
<li>随训练数据集尺寸增大，建立模型的分类精度也会随之增大。</li>
<li>当训练数据集尺寸增大到一定程度时，建立模型的精度不会再持续增大，且最大分类精度不会超过模型对训练数据的拟合度。</li>
</ul>
</li>
<li>测试数据集大小与模型精度的关系：<ul>
<li>当测试数据集过小时，所测模型精度不具有代表性，没有参考价值。</li>
<li>随测试数据集尺寸增大，模型精度也会随之增大。</li>
<li>当测试数据集尺寸增大到一定程度时，对模型精度的测量值不会再持续增大，并保持在某一数值上下微小浮动。</li>
</ul>
</li>
<li>属性个数对数据集大小与模型精度的关系的影响：<ul>
<li>当实例的属性个数过少时，所建模型精度低，没有参考价值。</li>
<li>随实例的属性个数增多，所建立模型的精度也会随之增大。</li>
</ul>
</li>
</ul>
<p>通过本次实验透彻的了解了决策树各种构造算法和剪枝算法，通过使用 Python 进行实现基本的决策树和简单的剪枝算法，锻炼了我的代码能力和对相应伪代码的理解能力。</p>
<p>在一开始，我对于决策树的整体没有认识，不知道如何从零开始构建一棵决策树，通过仔细研究课本，将思路从认识整体调整为模拟数据的流向。<br>首先定义参与到结构中的决策树的数据格式为 list 数组类型的嵌套。然后思考如何构建一棵决策树，自然而然想到的就是递归，通过返回节点的实例来递归建树。<br>下面碰到的问题就在于如何有机统一不同特征的不同的取值可能和在推理时的便利实现相结合，那么 Python 内建的 dict 字典类型就是很好的选择，通过设置特征的不同的可能取值为 key 值，对应的 value 为递归返回的结果，若连接的为叶节点，则 value 为 ‘0’ 或 ‘1’ 表示正负例，若为决策树的内部节点，则储存对应的对象，将查询请求递归处理直到叶节点。<br>下来就是漫长的 debug 时间，通过 pyCharm 方便的断点，我能成功复现每个异常发生时的程序情况，思考为什么会发生当前的状况和怎样进行修改。一开始我在实现后剪枝是还是给整个函数传入一整棵树，先递归到叶节点在向上逐层剪枝。但是在我仔细研究课本过后，发现书上给的顺序本就是自底向上的，所以只需要在建树时先剪枝再返回节点即可。</p>
<p>通过实现决策树和两个剪枝算法，我懂得了实际生活中，专业知识是怎样应用与实践的。同时了解了所谓的复现是什么，通过别人的描述重现他人的工作。了解到在实验中应该先彻底明白算法再进行复现，否则可能因为理解的偏差很多工作可能是错误的。为了最大化效率，应该先明确每个实现细节，在确定完了细节之后从下向上构建代码。</p>

    </div>

    
    
    
<div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------以上<i class="fa fa-paw"></i>（或许会有不定时更新）-------------</div>
    
</div>

  
</div>
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Komeiji Satori
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://kome1jisatori.github.io/2021/07/17/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A/" title="pytorch线性回归">http://kome1jisatori.github.io/2021/07/17/决策树实验报告/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/17/pytorch/" rel="prev" title="Tensor性质和用法">
      <i class="fa fa-chevron-left"></i> Tensor性质和用法
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/17/pytorch2/" rel="next" title="pytorch线性回归">
      pytorch线性回归 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A"><span class="nav-text">决策树实验报告</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E8%A6%81%E6%B1%82"><span class="nav-text">实验要求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-text">原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ID3"><span class="nav-text">ID3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C4-5"><span class="nav-text">C4.5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART"><span class="nav-text">CART</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%AA%E6%9E%9D"><span class="nav-text">剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D-pre-pruning"><span class="nav-text">预剪枝 pre-pruning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D-post-pruning"><span class="nav-text">后剪枝 post-pruning</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="nav-text">实现细节</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ID3-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="nav-text">ID3 算法实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C4-5-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="nav-text">C4.5 算法实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CART-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="nav-text">CART 算法实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BB%BA%E6%A0%91%E6%93%8D%E4%BD%9C"><span class="nav-text">建树操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D"><span class="nav-text">预剪枝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="nav-text">后剪枝</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="nav-text">实验结果分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A5%BF%E7%93%9C%E6%95%B0%E6%8D%AE%E9%9B%863-0"><span class="nav-text">西瓜数据集3.0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A5%BF%E7%93%9C%E6%95%B0%E6%8D%AE%E9%9B%862-0"><span class="nav-text">西瓜数据集2.0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UCI-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-text">UCI 数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Iris"><span class="nav-text">Iris</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Balloons"><span class="nav-text">Balloons</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AA%E4%BA%BA%E6%84%9F%E6%82%9F"><span class="nav-text">个人感悟</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Komeiji Satori"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Komeiji Satori</p>
  <div class="site-description" itemprop="description">记录些有的没的的学习心得</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/kome1jisatori" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;kome1jisatori" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=740775405&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;740775405&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      膜大佬的链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://sherlockcoder.xyz/" title="http:&#x2F;&#x2F;sherlockcoder.xyz" rel="noopener" target="_blank">不朽之存在的首页</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://datawhalechina.github.io/leeml-notes/#/" title="https:&#x2F;&#x2F;datawhalechina.github.io&#x2F;leeml-notes&#x2F;#&#x2F;" rel="noopener" target="_blank">机器学习笔记(LeeML-Notes)</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">苏ICP备 - 2021020302 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">KomeijiSatori</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">313k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">4:45</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '8NhM2OcimmX1hgpelhxOhQK8-gzGzoHsz',
      appKey     : 'bQ1303bPFHzcUrkoBzb39dgO',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>



  <script type="text/javascript" color="0,0,0" opacity='0.5' zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>


<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":true},"log":false});</script></body>
</html>
